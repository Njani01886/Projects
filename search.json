[
  {
    "objectID": "about/Nayan.html",
    "href": "about/Nayan.html",
    "title": "Nayan Jani",
    "section": "",
    "text": "Education\nUniversity of Massachusetts Amherst, College of Social and Behavioral Sciences, Amherst, MA. Exp. Graduation:: Feb 2024 Master of Science Concentration: Data Analytics and Computational Social Science Current GPA: 3.89 Relevant Courses: Advanced Quantitative Methods, Regression Models, Text as Data, Advanced Data-Driven Storytelling\nUniversity of Rhode Island, College of Arts and Sciences, Kingston, RI. Graduated:: May 2022 Bachelor of Science Concentration: Data Science Final GPA: 3.52, Dean’s List: 2018, 2019, 2020, 2021, 2022 Relevant Courses: Machine Learning, Multivariate Statistical Learning, Big Data Analysis, Database Management\n\n\nSkills\nGeneral Skills: Data Analysis, Machine Learning, Data Visualization, Data Storytelling, Data Cleaning, NLP\nProgramming Languages: Python, R, and SQL\nLibraries: Ggplot2, Tidyverse, Summarytools, Stats, Tidytext, Quanteda, Rselenium, Lubridate, Devtools, SciPy, Matplotlib, Seaborn, Scikit-learn, Numpy, Pandas\n\n\nProjects\nU.S. Job Satisfaction: Impacts of Technology, Work Environment, and Covid-19 September-December 2023\n\nInvestigated if the move to more work from home and more use of technology at work post-COVID actually lead to any significant changes in job satisfaction using data from the 2018 and 2022 versions of the General Social Survey.\nDeveloped two ordered logit GLMs using the MASS package in R: one main effect model and one interaction model\nCalculated predicted probabilities of job satisfaction for all significant variables (p <0.05) using ggpredict() and graphed them with 95% confidence intervals using ggplot().\nDiscovered that people who never work from home are significantly less likely to to be very satisfied with their job than people who mainly WFH (p <0.05), with all variables held constant.\n\nLink to paper: U.S. Job Satisfaction: Impacts of Technology, Work Environment, and Covid-19\nIdentifying Sources of Poor Nutrition for Americans, Amherst, MA July-August 2023 Project Owner - Analyzed the impact of food sources on food and nutrient intakes for different ages and income levels using 2017-18 Food and Nutrient Density by Food Source and Demographic Characteristics datasets - Created multiple charts that visualized the 3 way associations between food source, demographics and food/nutrient density using\nggplot2 in R - Discovered that adults and seniors were consuming more cholesterol at restaurants than at home, low and middle income individuals had lower intakes of protein at home than away from home - Cleaned and combined datasets using the tidyverse package in R in order to produce visualizations - Explained methods and results using language that a non-technical audience could understand in a 29 page report\nLink: Identifying Sources of Poor Nutrition for Americans\nFixing Social Media Design Brief February-May 2023 Team Member - Developed a design brief for an app called “Social Media Royale” that helps reduce people’s screen time with 2 student colleagues - Handled background research and interviews over Zoom to understand the consequences of high amounts of screen time - Brainstormed many design ideas that were is used in the final pitch with my team members - Gained substantial knowledge about the process of product design\nLink: Fixing Social Media Design Brief\nResearch Design Final Project February-May 2023 Team Member - Conducted a survey experiment research study with the goal of understanding if exposure to misinformation increases belief in conspiracy theories with two student colleagues - Defined constructs, operational definitions, ways of measurement for constructs, and treatment groups for the research study - Used Qualtrics for distributing the survey - Analyzed results from the survey using stacked bar charts, One-Way Anova, and Linear Regression using R\nLink: Misinformation and Conspiracy Theories\nNBA Salary Prediction, Amherst, MA February-May 2023 Project Owner - Developed and tested 4 machine learning models using different regression methods to explore which one performs the best at predicting NBA players’ salaries using R and Python - Found that Random Forest performed the best out of all methods, yielding a low RMSLE of 0.50 - Pre-processed and wrangled data into a suitable format for the ML models - Tuned hyperparameters for each model using GridSearchCV with 5 folds to control for overfitting - Presented my work at my program’s research symposium in front of faculty and peers\nLink: NBA Salary Prediction\nGoals in Soccer Regression Models Project, Amherst, MA November-December 2022\nTeam Member - Investigated the difference in total goals across Europe’s top 5 soccer leagues using team data from the 2021-22 season to understand if the type of league determines the number of goals per season - Developed a Quasi-Poisson regression model with 9 predictors using R with 3 student colleagues - Concluded that there was a significant difference in goals between Bundesliga and other leagues - Handled the the selection of predictors by creating and analyzing a scatter-plot matrix using GGally - Coordinated weekly group discussions with team members on Zoom and in person\nLink: Goals in Soccer\nAssessing Sentiment Surrounding the 2022 World Cup, Amherst, MA September-December 2022 Project Owner - Analyzed controversies of the Qatar World Cup by conducting Sentiment Analysis, LDA Topic Modeling, Semantic Network, and Pairwise\nCorrelation Analysis on Youtube comments using R - Concluded that the overall sentiment of the comments was negative, main focus of discussion surrounded human rights violations - Used packages tidyverse, quanteda, tidytext, text2vec, LDAvis, and sentimentr to conduct analysis - Extracted 1,391 comments in total from 9 Youtube videos using Youtube API in Python\nMy final conclusions were made on my final poster.\nFinal Poster: Assessing Sentiment Surrounding the 2022 World Cup\nSpring 2022 Machine Learning Project at URI, Kingston, RI Research Assistant - Designed a program in Python that estimated heterogeneous treatment effects using datasets that involve AIDS and breast cancer treatments as a part of my senior dissertation - Implemented different meta learners in my program that could estimate the conditional average treatment effect to identify how to personalize treatment regimes - Gained experience and knowledge in Causal Inference - Discussed methodology weekly with research leader\n\n\nExperience\nSummer 2021 Bora, La Jolla, CA Business Development & Land Acquisition - Compiled data about potential locations using Google Maps - Analyzed aggregated data to determine which locations are the busiest based on area busyness - Company’s goal is to bring to market an app based, self service beach chair rental service stationed at the most popular locations across the country - Part of outreach campaign to early stage VC and angel investors\nSpring 2018 Westford Academy Spring Recreational Basketball, Westford, MA Youth Basketball Coach/League Co-Director - Organized and operated a Spring League consisting of 80 players with 3 student colleagues as part of my senior year internship - Successfully coordinated a three month league for the Westford Youth Association. All surplus proceeds were donated back to the Westford Academy Girls Basketball Program - Established online sign-up and jersey purchases - Coordinated facilities and scheduling of refereeing staff\n\n\nHometown\nWestford, MA"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Contributors",
    "section": "",
    "text": "Nayan Jani\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DACSS 601 August 2022",
    "section": "",
    "text": "NBA Salary Prediction\n\n\n\n\n\n\n\nNBA\n\n\nMachine Learning\n\n\n\n\nMachine Learning for the Social Sciences\n\n\n\n\n\n\nJun 12, 2023\n\n\nNayan Jani\n\n\n\n\n\n\n\n\nU.S. Job Satisfaction: Impacts of Tech, Remote Work & Covid\n\n\n\n\n\n\n\nJob Satisfaction\n\n\nAdvanced Quant\n\n\n\n\n\n\n\n\n\n\n\nJan 8, 2014\n\n\nNayan Jani\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/NBA_Salary.html",
    "href": "posts/NBA_Salary.html",
    "title": "NBA Salary Prediction",
    "section": "",
    "text": "Code\nlibrary(alr4)\nlibrary(tinytex)\nlibrary(summarytools)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(splines)\nlibrary(aod)\nlibrary(DescTools)\nlibrary(MASS)\nlibrary(leaps)\nlibrary(GGally)\nlibrary(hrbrthemes)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/NBA_Salary.html#introduction",
    "href": "posts/NBA_Salary.html#introduction",
    "title": "NBA Salary Prediction",
    "section": "Introduction",
    "text": "Introduction\nBeing a General Manager in the NBA comes with a ton of decisions. One of the most important decisions a GM can make is how much they pay the players on their team. It is so important to pay the players the right amount in order to build the strongest roster. Overpaying a player will hurt a teams cap space, meaning that the team will not be able to sign good players because they do not have enough money to afford them. My motivation for this project its to see if Machine Learning techniques can correctly predict a players salary. The idea is if I am able to create a model that performs well enough, then it could be used as a tool to determine a players salary for their next contract. Here I will perform different regression methods to predict players salary and then used the best method for prediction."
  },
  {
    "objectID": "posts/NBA_Salary.html#the-data",
    "href": "posts/NBA_Salary.html#the-data",
    "title": "NBA Salary Prediction",
    "section": "The Data",
    "text": "The Data\nThe dataset I am using comes from Kaggle. The dataset contains information about player names, time span of the contract, avg salary per year and all stats that player accumulated during NBA season before signing their next contract. The scope of the data is as follows: - There are only contracts signed since 2010/2011 season to 2019/2020 season. - Only includes players that are active in 2020/2021 season. - Doesn’t include rookie or retained contracts. - Doesn’t include contracts for player that haven’t played year before the signing the contract.\nThis is a good scope because I want to use modern players contracts for future predictions. The limitation of only including players that are active in 20/21 means that these players were able to earn multiple contracts of the 10 year span, which validates them as players who are worth to continuing paying. Having this removes players who had massive contracts early in their career and then faded out quickly after their primes.\n\n\nCode\ndf<- read_csv(\"_data/nba_contracts_history.csv\")\n\n\nRows: 199 Columns: 28\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): NAME\ndbl (27): CONTRACT_START, CONTRACT_END, AVG_SALARY, AGE, GP, W, L, MIN, PTS,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\ndf\n\n\n# A tibble: 199 × 28\n   NAME  CONTR…¹ CONTR…² AVG_S…³   AGE    GP     W     L   MIN   PTS   FGM   FGA\n   <chr>   <dbl>   <dbl>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 Wesl…    2019    2020  2.56e6    32    69    27    42  2091   840   279   698\n 2 Broo…    2015    2017  2.12e7    27    72    34    38  2100  1236   506   987\n 3 DeAn…    2011    2014  1.08e7    22    80    31    49  2047   566   234   341\n 4 Mark…    2015    2018  8.14e6    25    82    39    43  2581  1258   512  1100\n 5 Dwig…    2018    2019  1.34e7    32    81    35    46  2463  1347   506   911\n 6 Aust…    2015    2016  7.06e6    22    76    46    30  1563   530   203   496\n 7 Wayn…    2016    2017  6.14e6    28    76    18    58  1615   586   218   561\n 8 JaMy…    2019    2020  4.77e6    29    65    31    34  1371   611   230   476\n 9 Kyle…    2015    2018  4.05e6    25    51    15    36   824   294   119   242\n10 Trev…    2014    2017  8   e6    28    77    41    36  2723  1107   389   853\n# … with 189 more rows, 16 more variables: `FG%` <dbl>, `3PM` <dbl>,\n#   `3PA` <dbl>, `3P%` <dbl>, FTM <dbl>, FTA <dbl>, `FT%` <dbl>, OREB <dbl>,\n#   DREB <dbl>, REB <dbl>, AST <dbl>, TOV <dbl>, STL <dbl>, BLK <dbl>,\n#   PF <dbl>, `+/-` <dbl>, and abbreviated variable names ¹​CONTRACT_START,\n#   ²​CONTRACT_END, ³​AVG_SALARY\n\n\n\n\nCode\ndf<- df %>% mutate(c_duration = CONTRACT_END -CONTRACT_START)\n\n\nAfter loading in the data I can see it has 199 instances with 29 features. Some features will be removed such as player name, CONTRACT_START and CONTRACT_end. After EDA I will remove any features that interfere with my regression analysis. The only variable mutation I did was c_duration, which is how long a players contract lasted."
  },
  {
    "objectID": "posts/NBA_Salary.html#eda",
    "href": "posts/NBA_Salary.html#eda",
    "title": "NBA Salary Prediction",
    "section": "EDA",
    "text": "EDA\n\n\nCode\nggplot(data=df, aes(x=AGE, y=AVG_SALARY)) +\n  geom_bar(stat=\"identity\") +\n  labs(title = \"Average Salary by Age\", x = \"Age\" ,y= \"Average Salary\")\n\n\n\n\n\nThe first relationship I wanted to visualize was Average salary and Age. Age is very important when thinking about how much to pay someone because you want to give them a contract that shows how good they can perform for that length of contract. I can see that average salary peaks at age 23 and again rises at 27 and then 30. This make sense because young players who are really good will receive a massive 2nd contract (contract after rookie deal). The peaks at 27 and 30 could be from players signing their 3rd or 4th contracts but make less money due to their age.\n\n\nCode\nprint(dfSummary(df, varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\ndf\nDimensions: 199 x 29\n  Duplicates: 1\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      NAME\n[character]\n      1. Kevin Durant2. Austin Rivers3. Avery Bradley4. Danilo Gallinari5. Dwight Howard6. E'Twaun Moore7. Paul Millsap8. Quinn Cook9. Robin Lopez10. Wayne Ellington[ 128 others ]\n      4(2.0%)3(1.5%)3(1.5%)3(1.5%)3(1.5%)3(1.5%)3(1.5%)3(1.5%)3(1.5%)3(1.5%)168(84.4%)\n      \n      0\n(0.0%)\n    \n    \n      CONTRACT_START\n[numeric]\n      Mean (sd) : 2015.2 (2.1)min ≤ med ≤ max:2011 ≤ 2015 ≤ 2019IQR (CV) : 3 (0)\n      2011:9(4.5%)2012:16(8.0%)2013:16(8.0%)2014:21(10.6%)2015:48(24.1%)2016:35(17.6%)2017:23(11.6%)2018:20(10.1%)2019:11(5.5%)\n      \n      0\n(0.0%)\n    \n    \n      CONTRACT_END\n[numeric]\n      Mean (sd) : 2017.5 (1.7)min ≤ med ≤ max:2013 ≤ 2018 ≤ 2020IQR (CV) : 3 (0)\n      2013:2(1.0%)2014:11(5.5%)2015:18(9.0%)2016:23(11.6%)2017:29(14.6%)2018:46(23.1%)2019:55(27.6%)2020:15(7.5%)\n      \n      0\n(0.0%)\n    \n    \n      AVG_SALARY\n[numeric]\n      Mean (sd) : 11073609 (7897820)min ≤ med ≤ max:823244 ≤ 9500000 ≤ 33599500IQR (CV) : 11621898 (0.7)\n      172 distinct values\n      \n      0\n(0.0%)\n    \n    \n      AGE\n[numeric]\n      Mean (sd) : 25.9 (2.8)min ≤ med ≤ max:20 ≤ 25 ≤ 36IQR (CV) : 4 (0.1)\n      16 distinct values\n      \n      0\n(0.0%)\n    \n    \n      GP\n[numeric]\n      Mean (sd) : 64.2 (19.6)min ≤ med ≤ max:1 ≤ 72 ≤ 82IQR (CV) : 19 (0.3)\n      59 distinct values\n      \n      0\n(0.0%)\n    \n    \n      W\n[numeric]\n      Mean (sd) : 34.2 (14.5)min ≤ med ≤ max:0 ≤ 35 ≤ 64IQR (CV) : 20 (0.4)\n      56 distinct values\n      \n      0\n(0.0%)\n    \n    \n      L\n[numeric]\n      Mean (sd) : 30 (13)min ≤ med ≤ max:0 ≤ 31 ≤ 62IQR (CV) : 18.5 (0.4)\n      53 distinct values\n      \n      0\n(0.0%)\n    \n    \n      MIN\n[numeric]\n      Mean (sd) : 1747 (782.4)min ≤ med ≤ max:2 ≤ 1867 ≤ 3125IQR (CV) : 1125.5 (0.4)\n      193 distinct values\n      \n      0\n(0.0%)\n    \n    \n      PTS\n[numeric]\n      Mean (sd) : 813.4 (499.9)min ≤ med ≤ max:0 ≤ 734 ≤ 2376IQR (CV) : 710.5 (0.6)\n      186 distinct values\n      \n      0\n(0.0%)\n    \n    \n      FGM\n[numeric]\n      Mean (sd) : 300.4 (178.6)min ≤ med ≤ max:0 ≤ 277 ≤ 743IQR (CV) : 261 (0.6)\n      172 distinct values\n      \n      0\n(0.0%)\n    \n    \n      FGA\n[numeric]\n      Mean (sd) : 638.8 (374.6)min ≤ med ≤ max:1 ≤ 572 ≤ 1643IQR (CV) : 553.5 (0.6)\n      189 distinct values\n      \n      0\n(0.0%)\n    \n    \n      FG%\n[numeric]\n      Mean (sd) : 46.7 (8.1)min ≤ med ≤ max:0 ≤ 45.5 ≤ 100IQR (CV) : 7 (0.2)\n      131 distinct values\n      \n      0\n(0.0%)\n    \n    \n      3PM\n[numeric]\n      Mean (sd) : 63.6 (58.4)min ≤ med ≤ max:0 ≤ 57 ≤ 272IQR (CV) : 91 (0.9)\n      111 distinct values\n      \n      0\n(0.0%)\n    \n    \n      3PA\n[numeric]\n      Mean (sd) : 173.9 (148.7)min ≤ med ≤ max:0 ≤ 170 ≤ 657IQR (CV) : 241 (0.9)\n      139 distinct values\n      \n      0\n(0.0%)\n    \n    \n      3P%\n[numeric]\n      Mean (sd) : 29.7 (13.2)min ≤ med ≤ max:0 ≤ 34.8 ≤ 50IQR (CV) : 11.9 (0.4)\n      110 distinct values\n      \n      0\n(0.0%)\n    \n    \n      FTM\n[numeric]\n      Mean (sd) : 149 (128.4)min ≤ med ≤ max:0 ≤ 107 ≤ 720IQR (CV) : 143 (0.9)\n      143 distinct values\n      \n      0\n(0.0%)\n    \n    \n      FTA\n[numeric]\n      Mean (sd) : 195.5 (162.2)min ≤ med ≤ max:0 ≤ 145 ≤ 837IQR (CV) : 192.5 (0.8)\n      162 distinct values\n      \n      0\n(0.0%)\n    \n    \n      FT%\n[numeric]\n      Mean (sd) : 74 (15)min ≤ med ≤ max:0 ≤ 76.8 ≤ 100IQR (CV) : 11.4 (0.2)\n      147 distinct values\n      \n      0\n(0.0%)\n    \n    \n      OREB\n[numeric]\n      Mean (sd) : 79.3 (72.7)min ≤ med ≤ max:0 ≤ 51 ≤ 397IQR (CV) : 85.5 (0.9)\n      117 distinct values\n      \n      0\n(0.0%)\n    \n    \n      DREB\n[numeric]\n      Mean (sd) : 249 (164)min ≤ med ≤ max:1 ≤ 209 ≤ 829IQR (CV) : 235 (0.7)\n      167 distinct values\n      \n      0\n(0.0%)\n    \n    \n      REB\n[numeric]\n      Mean (sd) : 328.3 (226.1)min ≤ med ≤ max:1 ≤ 286 ≤ 1226IQR (CV) : 305 (0.7)\n      169 distinct values\n      \n      0\n(0.0%)\n    \n    \n      AST\n[numeric]\n      Mean (sd) : 171.7 (163.9)min ≤ med ≤ max:0 ≤ 110 ≤ 839IQR (CV) : 137 (1)\n      147 distinct values\n      \n      0\n(0.0%)\n    \n    \n      TOV\n[numeric]\n      Mean (sd) : 103.4 (70.9)min ≤ med ≤ max:0 ≤ 85 ≤ 374IQR (CV) : 83 (0.7)\n      138 distinct values\n      \n      0\n(0.0%)\n    \n    \n      STL\n[numeric]\n      Mean (sd) : 58.2 (37.3)min ≤ med ≤ max:0 ≤ 48 ≤ 169IQR (CV) : 43 (0.6)\n      100 distinct values\n      \n      0\n(0.0%)\n    \n    \n      BLK\n[numeric]\n      Mean (sd) : 39.6 (43.3)min ≤ med ≤ max:0 ≤ 23 ≤ 269IQR (CV) : 40 (1.1)\n      83 distinct values\n      \n      0\n(0.0%)\n    \n    \n      PF\n[numeric]\n      Mean (sd) : 137.7 (64.3)min ≤ med ≤ max:1 ≤ 137 ≤ 291IQR (CV) : 84 (0.5)\n      135 distinct values\n      \n      0\n(0.0%)\n    \n    \n      +/-\n[numeric]\n      Mean (sd) : 62.9 (227.7)min ≤ med ≤ max:-628 ≤ 44 ≤ 839IQR (CV) : 257.5 (3.6)\n      171 distinct values\n      \n      0\n(0.0%)\n    \n    \n      c_duration\n[numeric]\n      Mean (sd) : 2.3 (1.1)min ≤ med ≤ max:1 ≤ 2 ≤ 4IQR (CV) : 2 (0.5)\n      1:69(34.7%)2:36(18.1%)3:66(33.2%)4:28(14.1%)\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.0)2024-01-08\n\n\n\nHere I created summary statistics of the dataset using SummaryTools. The two stats that stood out to me was the mean games played and mean minutes played. The GP and MIN mean values are 64.2 and 1747, respectively. This implies that in order to be considered for another contract, players must play most of the season and play about 21 minutes per game. I would say that if you looked at all the means and medians of each feature, then those values represent a player who will get another NBA contract.\n\n\nCode\nggplot(data = df, mapping = aes(x = MIN, y = AVG_SALARY)) + \n  geom_point() +\n  geom_smooth() +\n  labs(title = \"Average Salary by Minutes Played\", x = \"Mins\" ,y= \"Average Salary\")\n\n\n\n\n\nNext I wanted to check the relationship between Minutes played and Average Salary. I can see that their relationship is non-linear but positive. I see an increase of salary once a player is playing roughly 1750 minutes but drops off around 2500 minutes.The positive relationship could suggest that minutes played can be a good predictor.\n\n\nCode\nggplot(data = df, mapping = aes(x = `+/-`, y = AVG_SALARY)) + \n  geom_point() +\n  geom_smooth() +\n  labs(title = \"Average Salary by Plus/Minus\", x = \"+/-\" ,y= \"Average Salary\")\n\n\n\n\n\nHere I wanted to see the relationship between +/- and Average salary. +/- is a sports statistic used to measure a player’s impact on the game, represented by the difference between their team’s total scoring versus their opponent’s when the player is in the game. I can see the relationship between +/- and Average Salary is non linear. I can see that there are players with awful +/- that are getting payed more than players with high +/-. I also see a lot of data grouped around 0 with low salaries, meaning these players probably did not play much.\n\n\nCode\ndf %>% filter(`+/-` < -300)\n\n\n# A tibble: 7 × 29\n  NAME   CONTR…¹ CONTR…² AVG_S…³   AGE    GP     W     L   MIN   PTS   FGM   FGA\n  <chr>    <dbl>   <dbl>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 Wayne…    2016    2017  6.14e6    28    76    18    58  1615   586   218   561\n2 JaVal…    2012    2015  1.13e7    24    61    22    39  1535   691   307   552\n3 Gordo…    2014    2017  1.90e7    24    77    23    54  2800  1248   426  1032\n4 Jorda…    2016    2019  1.25e7    24    79    17    62  2552  1225   475  1098\n5 Derri…    2014    2017  1.2 e7    22    73    25    48  2201   970   390   747\n6 JaKar…    2015    2016  1.10e6    22    74    17    57  1131   386   146   346\n7 Nikol…    2015    2018  1.2 e7    24    74    21    53  2529  1428   631  1206\n# … with 17 more variables: `FG%` <dbl>, `3PM` <dbl>, `3PA` <dbl>, `3P%` <dbl>,\n#   FTM <dbl>, FTA <dbl>, `FT%` <dbl>, OREB <dbl>, DREB <dbl>, REB <dbl>,\n#   AST <dbl>, TOV <dbl>, STL <dbl>, BLK <dbl>, PF <dbl>, `+/-` <dbl>,\n#   c_duration <dbl>, and abbreviated variable names ¹​CONTRACT_START,\n#   ²​CONTRACT_END, ³​AVG_SALARY\n\n\nHere I investigate the players with terrible +/-. Some of these players are getting paid well but their teams are so bad that their +/- statistic is negative. This makes me believe that non-linear regression methods would be useful if this feature is used for prediction of salary.\n\n\nCode\ndf %>% dplyr::select(-c(NAME,CONTRACT_START,CONTRACT_END)) %>% \n  ggpairs(columns=c(1, 2:7),\n          upper=list(continuous=wrap('cor',size=5)),\n          lower=list(combo=wrap(\"facethist\",bins=30)),\n          diag=list(continuous=wrap(\"densityDiag\"),alpha=0.5))\n\n\n\n\n\n\n\nCode\ndf %>% dplyr::select(-c(NAME,CONTRACT_START,CONTRACT_END)) %>% \n  ggpairs(columns=c(1, 8:13),\n          upper=list(continuous=wrap('cor',size=5)),\n          lower=list(combo=wrap(\"facethist\",bins=30)),\n          diag=list(continuous=wrap(\"densityDiag\"),alpha=0.5))\n\n\n\n\n\n\n\nCode\ndf %>% dplyr::select(-c(NAME,CONTRACT_START,CONTRACT_END)) %>% \n  ggpairs(columns=c(1, 22:26),\n          upper=list(continuous=wrap('cor',size=5)),\n          lower=list(combo=wrap(\"facethist\",bins=30)),\n          diag=list(continuous=wrap(\"densityDiag\"),alpha=0.5))\n\n\n\n\n\n\n\nCode\ndf %>% dplyr::select(-c(NAME,CONTRACT_START,CONTRACT_END)) %>% \n  ggpairs(columns=c(1, 14:20),\n          upper=list(continuous=wrap('cor',size=5)),\n          lower=list(combo=wrap(\"facethist\",bins=30)),\n          diag=list(continuous=wrap(\"densityDiag\"),alpha=0.5))\n\n\n\n\n\n\n\nCode\nlibrary(ggcorrplot)\n\ndf %>% dplyr::select(-c(NAME,CONTRACT_START,CONTRACT_END)) %>% \n  cor() %>% \n  ggcorrplot(hc.order = TRUE, type = \"lower\",outline.col = \"white\",lab=TRUE, lab_size=1)\n\n\n\n\n\nHere I created 3 ggpairs plots and a correlation matrix to investigate the correlations between my target variable and and features. I am looking for strong and weak correlations. Based on the correlation matrix, I see a mixture or strong and weak correlations between features and my target variable, Average Salary. This makes me believe that non linear regression methods will perform better than linear regression methods on this dataset.\n\n\nCode\ndf %>%  ggplot(aes(x=AVG_SALARY)) + \n  geom_density() +\n  geom_vline(aes(xintercept=mean(AVG_SALARY)),\n            color=\"blue\", linetype=\"dashed\", size=1)\n\n\n\n\n\nCode\ndf %>% dplyr::select(AVG_SALARY) %>% \n  summarise(mean = mean(AVG_SALARY))\n\n\n# A tibble: 1 × 1\n       mean\n      <dbl>\n1 11073609.\n\n\nHere I wanted to look at the distribution of my target variable. I can see the distribution is not normal and skewed right. Seeing this makes me think that more flexible methods will work better when I create my regression models."
  },
  {
    "objectID": "posts/NBA_Salary.html#pre-processing-in-python",
    "href": "posts/NBA_Salary.html#pre-processing-in-python",
    "title": "NBA Salary Prediction",
    "section": "Pre Processing in Python",
    "text": "Pre Processing in Python\n\n\nCode\ndf = pd.read_csv(\"_data/nba_contracts_history.csv\")\n\n\n\n\nCode\n#Creating variable c_duration\ndf[\"c_duration\"] = df[\"CONTRACT_END\"] - df[\"CONTRACT_START\"] \n\n\n\n\nCode\n#Creates set of features I will use\nfeatures = df.drop([\"NAME\",\"CONTRACT_START\", \"CONTRACT_END\",\"AVG_SALARY\",\"W\",\"L\"], axis=1)\n\n\n\n\nCode\n#sub-setting my target variable, Average Salary\ntarget = df['AVG_SALARY'].to_numpy()\n\n\nHere is my data splits, 70% training, 15% validation and 15% Test.\n\n\nCode\nX_train, X_test, Y_train, Y_test = train_test_split(features, target, test_size=0.3, random_state=42)\nX_valid, X_test, Y_valid, Y_test = train_test_split(X_test, Y_test, test_size=0.5, random_state= 42)\n\n\nHere I check to see if the data is split up correctly.\n\n\nCode\nprint(X_train.shape, X_valid.shape, X_test.shape)\n\n\n(139, 23) (30, 23) (30, 23)\n\n\nCode\nprint(Y_train.shape, Y_valid.shape, Y_test.shape)\n\n\n(139,) (30,) (30,)\n\n\nHere I use the make_scorer() function so that I can call RMSLE during Grid Search Cross Validation.\n\n\nCode\nRMSLE = make_scorer(mean_squared_log_error, squared=False)"
  },
  {
    "objectID": "posts/NBA_Salary.html#random-forest",
    "href": "posts/NBA_Salary.html#random-forest",
    "title": "NBA Salary Prediction",
    "section": "Random Forest",
    "text": "Random Forest\nFor hyperparameter tuning, I chose to tune n_estimators, max_features, min_samples_split and min_sample_leafs. From Sckit learn’s documentation, it states that n_estimators and max_features are the main parameters to tune, so I will follow the documentations suggestion. I included min_sample_split and min_sample_leafs because I want to control my model for overfitting. By increasing the values of these two parameters, it will help prevent my model from overfitting.\n\n\nCode\nfrom sklearn.ensemble import RandomForestRegressor\nRF = RandomForestRegressor()\n\nparam_grid_RF = {\n    'n_estimators': [100, 200, 300, 400],\n     'max_features': [None, 1.0],\n     'min_samples_split': [2, 4, 6 ,8],\n    'min_samples_leaf': [1, 2 ,4, 6, 8]}\n    \n    \n    \ngridRF= GridSearchCV(RF, param_grid_RF,scoring = RMSLE, cv=5)\ngridRF.fit(X_train, Y_train)\n\n\nGridSearchCV(cv=5, estimator=RandomForestRegressor(),\n             param_grid={'max_features': [None, 1.0],\n                         'min_samples_leaf': [1, 2, 4, 6, 8],\n                         'min_samples_split': [2, 4, 6, 8],\n                         'n_estimators': [100, 200, 300, 400]},\n             scoring=make_scorer(mean_squared_log_error, squared=False))\n\n\nCode\nprint(\"Best parameters:\",gridRF.best_params_)\n\n\nBest parameters: {'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n\n\nCode\nprint(\"Training error (RMSLE):\", gridRF.best_score_)\n\n\nTraining error (RMSLE): 0.5746711676740883\n\n\nChecking Validation Error for Random Forests.\n\n\nCode\n\nbest_model_RF = gridRF.best_estimator_\npredict_y_RF = best_model_RF.predict(X_valid)\n\nRF_rmsle = mean_squared_log_error(Y_valid, predict_y_RF,squared=False)\n\n\nprint(\"Validation error (RMSLE):\",RF_rmsle)\n\n\nValidation error (RMSLE): 0.45176163177300405"
  },
  {
    "objectID": "posts/NBA_Salary.html#scaling-data",
    "href": "posts/NBA_Salary.html#scaling-data",
    "title": "NBA Salary Prediction",
    "section": "Scaling Data",
    "text": "Scaling Data\nScaling data for SVM and Ridge Regression.\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_valid_scaled = scaler.transform(X_valid)\nX_test_scaled = scaler.transform(X_test)"
  },
  {
    "objectID": "posts/NBA_Salary.html#svr",
    "href": "posts/NBA_Salary.html#svr",
    "title": "NBA Salary Prediction",
    "section": "SVR",
    "text": "SVR\nThe hyperparameters I tuned from SVR are C, gamma and kernel. I needed to tune for Kernel so that my model finds the best hyper plane that fits the datapoints from my target variable. The kernel will deal with the non-linear relationship on between my features and target variable. Tuning for C and gamma will help my model from overfitting.\n\n\nCode\nfrom sklearn.svm import SVR\n\nSVR = SVR()\n\nparam_grid_SVR = {'C': [0.1, 1, 10, 100, 1000],\n              'gamma': [0.0001, 0.001,0.01, 0.1, 1],\n              'kernel': ['rbf', 'poly']}\ngridSVR= GridSearchCV(SVR, param_grid_SVR, scoring = RMSLE, cv=5)\ngridSVR.fit(X_train_scaled, Y_train)\n\n\nGridSearchCV(cv=5, estimator=SVR(),\n             param_grid={'C': [0.1, 1, 10, 100, 1000],\n                         'gamma': [0.0001, 0.001, 0.01, 0.1, 1],\n                         'kernel': ['rbf', 'poly']},\n             scoring=make_scorer(mean_squared_log_error, squared=False))\n\n\nCode\nprint(\"Best parameters:\",gridSVR.best_params_)\n\n\nBest parameters: {'C': 0.1, 'gamma': 0.0001, 'kernel': 'poly'}\n\n\nCode\nprint(\"Training error (RMSLE):\", gridSVR.best_score_)\n\n\nTraining error (RMSLE): 1.0135970157480874\n\n\nChecking Validation Error for SVR.\n\n\nCode\nbest_model_SVR = gridSVR.best_estimator_\npredict_y_SVR = best_model_SVR.predict(X_valid_scaled)\n\nSVR_rmsle = mean_squared_log_error(Y_valid, predict_y_SVR,squared=False)\n\n\nprint(\"Validation error (RMSLE):\", SVR_rmsle)\n\n\nValidation error (RMSLE): 0.6805718407616901"
  },
  {
    "objectID": "posts/NBA_Salary.html#ridge",
    "href": "posts/NBA_Salary.html#ridge",
    "title": "NBA Salary Prediction",
    "section": "Ridge",
    "text": "Ridge\nFor Ridge Regression, The only hyperparameter I tuned was alpha, which is the penalty term. I chose to search through small, intermediate, and large values of alpha.\n\n\nCode\nridge = Ridge()\nparam_grid_ridge = {'alpha':np.concatenate((np.arange(0.1,2,0.1), np.arange(2, 5, 0.5), np.arange(5, 105, 5)))}\ngridRidge = GridSearchCV(ridge, param_grid_ridge,scoring = RMSLE, cv=5)\ngridRidge.fit(X_train_scaled, Y_train)\n\n\nGridSearchCV(cv=5, estimator=Ridge(),\n             param_grid={'alpha': array([  0.1,   0.2,   0.3,   0.4,   0.5,   0.6,   0.7,   0.8,   0.9,\n         1. ,   1.1,   1.2,   1.3,   1.4,   1.5,   1.6,   1.7,   1.8,\n         1.9,   2. ,   2.5,   3. ,   3.5,   4. ,   4.5,   5. ,  10. ,\n        15. ,  20. ,  25. ,  30. ,  35. ,  40. ,  45. ,  50. ,  55. ,\n        60. ,  65. ,  70. ,  75. ,  80. ,  85. ,  90. ,  95. , 100. ])},\n             scoring=make_scorer(mean_squared_log_error, squared=False))\n\n\nCode\nprint(\"Best parameters:\", gridRidge.best_params_)\n\n\nBest parameters: {'alpha': 100.0}\n\n\nCode\nprint(\"Training error (RMSLE):\", gridRidge.best_score_)\n\n\nTraining error (RMSLE): 0.5579904916871834\n\n\nChecking Validation Error for Ridge Regression.\n\n\nCode\nbest_model_ridge = gridRidge.best_estimator_\npredict_y_ridge = best_model_ridge.predict(X_valid_scaled)\n\n\nridge_rmsle = mean_squared_log_error(Y_valid, predict_y_ridge,squared=False)\n\n\nprint(\"Validation error (RMSLE):\", ridge_rmsle)\n\n\nValidation error (RMSLE): 0.5200462148165859"
  },
  {
    "objectID": "posts/NBA_Salary.html#gradient-boosting-decision-trees-regressor",
    "href": "posts/NBA_Salary.html#gradient-boosting-decision-trees-regressor",
    "title": "NBA Salary Prediction",
    "section": "Gradient Boosting Decision Trees Regressor",
    "text": "Gradient Boosting Decision Trees Regressor\nThe hyperparameters I chose to tune for the Gradient Boosting Decision Trees Regressor are n_estimators, learning_rate, and max_depth. GBDTR uses multiple shallow decision trees as weak learners to predict the residuals of the decision trees instead of the target variable. The idea here is use gradient descent to update the residuals after every tree until the model has run through all available trees (n_estimators). Learning rate controls how much the residuals are updated from tree to tree, which can also be described as the “size” of the step in gradient descent. Smaller values of learning rate will allow my model to generalize better on the validation and test data. Max_depth refers to the depth of each tree, which is important because it determines how weak the trees are in the ensemble.\n\n\nCode\nfrom sklearn.ensemble import GradientBoostingRegressor\nGBR = GradientBoostingRegressor()\n\nparam_grid_GBR = {\n    'n_estimators': [100, 200, 300, 400],\n     'learning_rate': [ 0.01, 0.1, 0.2,0.3],\n     'max_depth': [3,4,5,6,7,8]}\n    \n    \n    \ngridGBR= GridSearchCV(GBR, param_grid_GBR,scoring = RMSLE, cv=5)\ngridGBR.fit(X_train, Y_train)\n\n\nGridSearchCV(cv=5, estimator=GradientBoostingRegressor(),\n             param_grid={'learning_rate': [0.01, 0.1, 0.2, 0.3],\n                         'max_depth': [3, 4, 5, 6, 7, 8],\n                         'n_estimators': [100, 200, 300, 400]},\n             scoring=make_scorer(mean_squared_log_error, squared=False))\n\n\nCode\nprint(\"Best parameters:\", gridGBR.best_params_)\n\n\nBest parameters: {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 100}\n\n\nCode\nprint(\"Training error (RMSLE):\", gridGBR.best_score_)\n\n\nTraining error (RMSLE): 0.7310364505797857\n\n\nChecking Validation Error for Gradient Boosting Decision Trees.\n\n\nCode\nbest_model_GBR = gridGBR.best_estimator_\npredict_y_GBR = best_model_GBR.predict(X_valid)\n\n\nGBR_rmsle = mean_squared_log_error(Y_valid, predict_y_GBR,squared=False)\n\n\nprint(\"Validation error (RMSLE):\", GBR_rmsle)\n\n\nValidation error (RMSLE): 0.5710744674413062"
  },
  {
    "objectID": "posts/NBA_Salary.html#final-evaluation-using-test-set",
    "href": "posts/NBA_Salary.html#final-evaluation-using-test-set",
    "title": "NBA Salary Prediction",
    "section": "Final Evaluation using Test Set",
    "text": "Final Evaluation using Test Set\nChecking Test Error for all models. Again I will list the best parameters selected from Grid Search CV.\n\n\nCode\n\nbest_model_RF\n\n\nRandomForestRegressor(max_features=None)\n\n\nCode\npredict_test_RF = best_model_RF.predict(X_test)\n\n\nRF_test_rmsle = mean_squared_log_error(Y_test, predict_test_RF,squared=False)\n\n\nprint(\"Test error (RMSLE):\", RF_test_rmsle)\n\n\nTest error (RMSLE): 0.49662805909554597\n\n\n\n\nCode\nbest_model_SVR \n\n\nSVR(C=0.1, gamma=0.0001, kernel='poly')\n\n\nCode\npredict_test_SVR = best_model_SVR.predict(X_test_scaled)\n\nSVR_test_rmsle = mean_squared_log_error(Y_test, predict_test_SVR,squared=False)\n\n\nprint(\"Test error (RMSLE):\", SVR_test_rmsle)\n\n\nTest error (RMSLE): 1.0137954511265557\n\n\n\n\nCode\nbest_model_ridge\n\n\nRidge(alpha=100.0)\n\n\nCode\npredict_test_ridge = best_model_ridge.predict(X_test_scaled)\n\n\nridge_test_rmsle = mean_squared_log_error(Y_test, predict_test_ridge,squared=False)\n\n\nprint(\"Test error (RMSLE):\", ridge_test_rmsle)\n\n\nTest error (RMSLE): 0.5376660192539188\n\n\n\n\nCode\n\nbest_model_GBR\n\n\nGradientBoostingRegressor(learning_rate=0.01, max_depth=6)\n\n\nCode\npredict_test_GBR = best_model_GBR.predict(X_test)\n\n\nGBR_test_rmsle = mean_squared_log_error(Y_test, predict_test_GBR,squared=False)\n\n\nprint(\"Test error (RMSLE):\", GBR_test_rmsle)\n\n\nTest error (RMSLE): 0.7873592182966553"
  },
  {
    "objectID": "posts/NJ_Adv_Quant.html",
    "href": "posts/NJ_Adv_Quant.html",
    "title": "U.S. Job Satisfaction: Impacts of Tech, Remote Work & Covid",
    "section": "",
    "text": "Code\nGSS_18<- read_dta(\"_data/GSS2018.dta\")\n\nGSS_22<- read_dta(\"_data/GSS2022.dta\")"
  },
  {
    "objectID": "posts/NJ_Adv_Quant.html#selecting-variables-to-use",
    "href": "posts/NJ_Adv_Quant.html#selecting-variables-to-use",
    "title": "U.S. Job Satisfaction: Impacts of Tech, Remote Work & Covid",
    "section": "Selecting variables to use",
    "text": "Selecting variables to use\nHere I renamed the different weight columns to ‘weights’ so that I could combine the datasets\n\n\n\nCombine datasets using rbind() and named it GSS\n\n\nCode\nGSS <-rbind(GSS_18,GSS_22)"
  },
  {
    "objectID": "posts/NJ_Adv_Quant.html#recoding-variables-and-removing-nas",
    "href": "posts/NJ_Adv_Quant.html#recoding-variables-and-removing-nas",
    "title": "U.S. Job Satisfaction: Impacts of Tech, Remote Work & Covid",
    "section": "Recoding variables and removing NAs",
    "text": "Recoding variables and removing NAs\nHere I recoded all my variables with new names. I flipped the scale of satjob1 so that the negative responses were coded as lower values and the positive responses were coded as higher values. Age_group, years_job, POC, and use_tech all had to be collapsed into smaller groups in order to run analysis. All binary variables were coded as 0/1. Work_type was coded as a categorical variable.\n\n\nCode\nGSS %>%                  \n  count(satjob1) %>%                             \n  mutate(percent = scales::percent(n / sum(n)))\n\n\n# A tibble: 5 × 3\n  satjob1                          n percent\n  <dbl+lbl>                    <int> <chr>  \n1     1 [very satisfied]        1616 27.4%  \n2     2 [somewhat satisfied]    1406 23.9%  \n3     3 [not too satisfied]      256 4.3%   \n4     4 [not at all satisfied]    82 1.4%   \n5 NA(i) [iap]                   2532 43.0%  \n\n\nCode\nGSS <- GSS %>% \n  mutate(job_sat = case_when(\n    satjob1 ==1 ~ 4,\n    satjob1 ==2 ~ 3,\n    satjob1 ==3 ~ 2,\n    satjob1 ==4 ~ 1,\n    satjob1 < 0 ~ NA_real_))\n\nGSS %>%                \n  count(job_sat) %>%                            \n  mutate(percent = scales::percent(n / sum(n)))\n\n\n# A tibble: 5 × 3\n  job_sat     n percent\n    <dbl> <int> <chr>  \n1       1    82 1.4%   \n2       2   256 4.3%   \n3       3  1406 23.9%  \n4       4  1616 27.4%  \n5      NA  2532 43.0%  \n\n\nCode\nGSS <- GSS %>% \n  mutate(wrkhome = case_when(\n    wrkhome ==1 ~ 1,\n    wrkhome ==2 ~ 2,\n    wrkhome ==3 ~ 3,\n    wrkhome ==4 ~ 4,\n    wrkhome ==5 ~ 5,\n    wrkhome ==6 ~ 6,\n    wrkhome < 0 ~ NA_real_))\n\nGSS <- GSS %>% \n  mutate(degree = case_when(\n    degree ==0 ~ 1,\n    degree ==1 ~ 2,\n    degree ==2 ~ 3,\n    degree ==3 ~ 4,\n    degree ==4 ~ 5,\n    degree < 0 ~ NA_real_))\n\n\nGSS <- GSS %>% \n  mutate(post_covid = case_when(\n    year ==2018 ~ 0,\n    year ==2022 ~ 1,\n    year < 0 ~ NA_real_))\n\n\n\nGSS<- GSS %>%\n  mutate(use_tech = case_when(\n    usetech >= 0 & usetech <= 25 ~ 1,\n    usetech > 25 & usetech <= 50 ~ 2,\n    usetech > 50 & usetech <= 75 ~ 3,\n    usetech > 75 ~ 4,\n  ))\n\nGSS %>%               \n  count(age) %>%                           \n  mutate(percent = scales::percent(n / sum(n)))\n\n\n# A tibble: 73 × 3\n   age           n percent\n   <dbl+lbl> <int> <chr>  \n 1 18           44 0.747% \n 2 19           55 0.933% \n 3 20           63 1.069% \n 4 21           73 1.239% \n 5 22           86 1.460% \n 6 23           82 1.392% \n 7 24           83 1.409% \n 8 25           88 1.494% \n 9 26           89 1.511% \n10 27           85 1.443% \n# … with 63 more rows\n\n\nCode\nGSS <- GSS %>%\n  mutate(age = replace(age, age <= -1, NA))\n\nage_breaks <- c(18, 30, 40, 50, 65, 100)\n\n# Create a new variable with age groups\nGSS$age_group <- cut(GSS$age, \n                      age_breaks, labels = c(1, 2, 3, 4, 5), \n                      include.lowest = TRUE)\n\n\n\nGSS <- GSS %>% \n  mutate(female = case_when(\n    sex ==1 ~ 0,\n    sex ==2 ~ 1,\n    sex <=-1 ~ NA_real_)) \n\nGSS %>%                 \n  count(wrktype) %>%                             \n  mutate(percent = scales::percent(n / sum(n)))\n\n\n# A tibble: 6 × 3\n  wrktype                                                        n percent\n  <dbl+lbl>                                                  <int> <chr>  \n1     1 [independent contractor/consultant/freelance worker]   434 7.37%  \n2     2 [on-call, work only when called to work]                82 1.39%  \n3     3 [paid by a temporary agency]                            35 0.59%  \n4     4 [work for contractor who provides workers/services]    115 1.95%  \n5     5 [regular, permanent employee]                         2704 45.89% \n6 NA(i) [iap]                                                 2522 42.80% \n\n\nCode\nGSS <- GSS %>%\n  mutate(work_type = replace(wrktype, (wrktype <= -1 | wrktype >= 99), NA))\nfreq(GSS$work_type)\n\n\n\n\n\nwork arrangement at main job \n      Frequency Percent Valid Percent\n1           434   7.366        12.878\n2            82   1.392         2.433\n3            35   0.594         1.039\n4           115   1.952         3.412\n5          2704  45.893        80.237\nNA's       2522  42.804              \nTotal      5892 100.000       100.000\n\n\nCode\nvalue_labels <- c(\"Independent contractor/consultant/freelance worker\", \"On-call, work only when called to work\", \n                  \"Paid by a temporary agency\", \"Work for contractor who provides workers/services\",\n                  \"Regular, permanent employee\") \n\n\n# Assign value labels to the Response variable\nGSS$work_type <- factor(GSS$work_type, levels = 1:5, labels = value_labels)\n\n\nGSS %>%                \n  count(race) %>%                            \n  mutate(percent = scales::percent(n / sum(n)))\n\n\n# A tibble: 4 × 3\n  race              n percent\n  <dbl+lbl>     <int> <chr>  \n1     1 [white]  4207 71.4%  \n2     2 [black]   950 16.1%  \n3     3 [other]   682 11.6%  \n4 NA(i) [iap]      53 0.9%   \n\n\nCode\nGSS <- GSS %>% \n  mutate(poc = case_when(\n    race==1 ~ 0,\n    race == 2 | race == 3 ~ 1,\n    race <=-1 ~ NA_real_)) \n\n\n\n\nGSS<- GSS %>%\n  mutate(years_job = case_when(\n    yearsjob >= 0 & yearsjob <= 1.0 ~ 1,\n    yearsjob > 1.0 & yearsjob <= 4 ~ 2,\n    yearsjob > 4 ~ 3,\n  ))\n\n\n\nGSS %>%                 \n  count(years_job) %>%                             \n  mutate(percent = scales::percent(n / sum(n)))\n\n\n# A tibble: 4 × 3\n  years_job     n percent\n      <dbl> <int> <chr>  \n1         1  1017 17.3%  \n2         2   802 13.6%  \n3         3  1556 26.4%  \n4        NA  2517 42.7%"
  },
  {
    "objectID": "posts/NJ_Adv_Quant.html#dv-distribution",
    "href": "posts/NJ_Adv_Quant.html#dv-distribution",
    "title": "U.S. Job Satisfaction: Impacts of Tech, Remote Work & Covid",
    "section": "DV distribution",
    "text": "DV distribution\nHere is a graph of the distribution of my DV.\n\n\nCode\ncustom_order <- c('Not at all satisfied', 'Not too satisfied', 'Somewhat satisfied', 'Very satisfied')\n\nggplot(GSS, aes(x = job_sat)) +\n  geom_density(fill = \"blue\", alpha = 0.5) +\n  labs(x = \"Value\", y = \"Density\", title = \"Distribution of Job Satisfaction\")  + \n  scale_x_discrete(limits = custom_order)\n\n\nWarning: Removed 2532 rows containing non-finite values (`stat_density()`).\n\n\n\n\n\nGraph suggested that an ordered model would be most appropriate for my models."
  },
  {
    "objectID": "posts/NJ_Adv_Quant.html#check-to-see-if-there-is-a-nested-data-structure-based-on-the-grouping-variable",
    "href": "posts/NJ_Adv_Quant.html#check-to-see-if-there-is-a-nested-data-structure-based-on-the-grouping-variable",
    "title": "U.S. Job Satisfaction: Impacts of Tech, Remote Work & Covid",
    "section": "Check to see if there is a nested data structure based on the grouping variable",
    "text": "Check to see if there is a nested data structure based on the grouping variable\nWork type is a grouping variable, so I checked to see if there is a nested data structure.\n\n\nCode\nGSS$wrktype <- as.character(GSS$wrktype)\n\nmlm <- lmer(job_sat ~ 1 + (1|wrktype), data = GSS, weights = weights)\nsummary(mlm)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: job_sat ~ 1 + (1 | wrktype)\n   Data: GSS\nWeights: weights\n\nREML criterion at convergence: 8212\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-12.1296  -0.4674  -0.2270   0.7235   2.7186 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n wrktype  (Intercept) 0.02348  0.1532  \n Residual             0.52486  0.7245  \nNumber of obs: 3321, groups:  wrktype, 5\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  3.39041    0.07508   45.16\n\n\nCode\nicc <- 0.02348/(0.02348 +0.52486)\n\nprint(icc)\n\n\n[1] 0.04282015\n\n\nICC < .1 indicates that there is not multilevel data structure within my dataset."
  },
  {
    "objectID": "posts/NJ_Adv_Quant.html#changing-the-data-types-for-important-variables",
    "href": "posts/NJ_Adv_Quant.html#changing-the-data-types-for-important-variables",
    "title": "U.S. Job Satisfaction: Impacts of Tech, Remote Work & Covid",
    "section": "Changing the data types for important variables",
    "text": "Changing the data types for important variables\nFirst, job_sat must be set as an ordered factor so that ordered logistic regression can be run. post_covid has to be set as a factor in order to run the interaction model. Use_tech_f and wrkhome_f are factor versions of use_tech and wrkhome. I set them to factors to investigate if they have a linear relationship with job satisfaction. Age_group is set as a numeric because it is a control variable. Finally I filter out any NAs again to ensure that all my data instances are clean.\n\n\nCode\nGSS$job_sat <- factor(GSS$job_sat, ordered = TRUE)\nGSS$post_covid <- factor(GSS$post_covid)\nGSS$use_tech_f <- factor(GSS$use_tech)\nGSS$wrkhome_f <- factor(GSS$wrkhome)\nGSS$age_group <- as.numeric(GSS$age_group)\n\nGSS <- GSS %>% \n  filter(complete.cases(.))"
  },
  {
    "objectID": "posts/NJ_Adv_Quant.html#check-to-see-if-use_tech-is-linear",
    "href": "posts/NJ_Adv_Quant.html#check-to-see-if-use_tech-is-linear",
    "title": "U.S. Job Satisfaction: Impacts of Tech, Remote Work & Covid",
    "section": "Check to see if use_tech is linear",
    "text": "Check to see if use_tech is linear\n\n\nCode\njs_l<-polr(job_sat ~ use_tech_f + wrkhome + post_covid + female + poc + age_group + work_type + years_job, data=GSS, na.action = na.exclude, method = \"logistic\", weights= weights )\n\n\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!\n\n\nCode\nstargazer(js_l, type=\"text\",\n          digits=2)\n\n\n\n======================================================================================\n                                                               Dependent variable:    \n                                                           ---------------------------\n                                                                     job_sat          \n--------------------------------------------------------------------------------------\nuse_tech_f2                                                          -0.17*           \n                                                                     (0.10)           \n                                                                                      \nuse_tech_f3                                                           0.01            \n                                                                     (0.12)           \n                                                                                      \nuse_tech_f4                                                         -0.37***          \n                                                                     (0.09)           \n                                                                                      \nwrkhome                                                              0.07***          \n                                                                     (0.02)           \n                                                                                      \npost_covid1                                                           -0.06           \n                                                                     (0.07)           \n                                                                                      \nfemale                                                                -0.05           \n                                                                     (0.07)           \n                                                                                      \npoc                                                                  -0.15*           \n                                                                     (0.08)           \n                                                                                      \nage_group                                                            0.22***          \n                                                                     (0.03)           \n                                                                                      \nwork_typeOn-call, work only when called to work                       -0.20           \n                                                                     (0.24)           \n                                                                                      \nwork_typePaid by a temporary agency                                 -1.63***          \n                                                                     (0.36)           \n                                                                                      \nwork_typeWork for contractor who provides workers/services            -0.33           \n                                                                     (0.22)           \n                                                                                      \nwork_typeRegular, permanent employee                                -0.55***          \n                                                                     (0.11)           \n                                                                                      \nyears_job                                                            -0.09**          \n                                                                     (0.04)           \n                                                                                      \n--------------------------------------------------------------------------------------\nObservations                                                          3,268           \n======================================================================================\nNote:                                                      *p<0.1; **p<0.05; ***p<0.01\n\n\nThe non linear relationship suggests that use_tech should be coded as a factor."
  },
  {
    "objectID": "posts/NJ_Adv_Quant.html#check-to-see-if-wrkhome-is-linear",
    "href": "posts/NJ_Adv_Quant.html#check-to-see-if-wrkhome-is-linear",
    "title": "U.S. Job Satisfaction: Impacts of Tech, Remote Work & Covid",
    "section": "Check to see if wrkhome is linear",
    "text": "Check to see if wrkhome is linear\nNon linear relationship suggests that wrkhome should be coded as a factor.\n\n\nCode\njs_l<-polr(job_sat ~ use_tech + wrkhome_f + post_covid + female + poc + age_group + work_type + years_job, data=GSS, na.action = na.exclude, method = \"logistic\", weights= weights )\n\n\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!\n\n\nCode\nstargazer(js_l, type=\"text\",\n         digits=2)\n\n\n\n======================================================================================\n                                                               Dependent variable:    \n                                                           ---------------------------\n                                                                     job_sat          \n--------------------------------------------------------------------------------------\nuse_tech                                                            -0.12***          \n                                                                     (0.03)           \n                                                                                      \nwrkhome_f2                                                            0.13            \n                                                                     (0.14)           \n                                                                                      \nwrkhome_f3                                                           0.54***          \n                                                                     (0.16)           \n                                                                                      \nwrkhome_f4                                                            0.26*           \n                                                                     (0.14)           \n                                                                                      \nwrkhome_f5                                                           0.27**           \n                                                                     (0.11)           \n                                                                                      \nwrkhome_f6                                                           0.33***          \n                                                                     (0.12)           \n                                                                                      \npost_covid1                                                           -0.05           \n                                                                     (0.07)           \n                                                                                      \nfemale                                                                -0.03           \n                                                                     (0.07)           \n                                                                                      \npoc                                                                  -0.14*           \n                                                                     (0.08)           \n                                                                                      \nage_group                                                            0.22***          \n                                                                     (0.03)           \n                                                                                      \nwork_typeOn-call, work only when called to work                       -0.18           \n                                                                     (0.24)           \n                                                                                      \nwork_typePaid by a temporary agency                                 -1.62***          \n                                                                     (0.36)           \n                                                                                      \nwork_typeWork for contractor who provides workers/services            -0.35           \n                                                                     (0.22)           \n                                                                                      \nwork_typeRegular, permanent employee                                -0.55***          \n                                                                     (0.11)           \n                                                                                      \nyears_job                                                            -0.10**          \n                                                                     (0.04)           \n                                                                                      \n--------------------------------------------------------------------------------------\nObservations                                                          3,268           \n======================================================================================\nNote:                                                      *p<0.1; **p<0.05; ***p<0.01\n\n\nThe non linear relationship suggests that wrkhome should be coded as a factor."
  },
  {
    "objectID": "posts/NJ_Adv_Quant.html#hypotheses",
    "href": "posts/NJ_Adv_Quant.html#hypotheses",
    "title": "U.S. Job Satisfaction: Impacts of Tech, Remote Work & Covid",
    "section": "Hypotheses",
    "text": "Hypotheses\n1: People who work mainly from home are more satisfied with their job than those who never work from home. 2: Working from home became more influential at making people more satisfied with their job post covid than pre covid. 3: In 2022, the working population is more satisfied with their jobs than they were in 2018. 4: People who are using technology for more than 75% of their total work time during a week are less satisfied with their jobs than others. 5: In post COVID times, using technology is related to lower job satisfaction than pre-COVID."
  },
  {
    "objectID": "posts/NJ_Adv_Quant.html#final-models",
    "href": "posts/NJ_Adv_Quant.html#final-models",
    "title": "U.S. Job Satisfaction: Impacts of Tech, Remote Work & Covid",
    "section": "Final Models",
    "text": "Final Models\nHere I change to weights to type as.double() and run my final models.\n\n\nCode\nGSS$weights <- as.double(GSS$weights)\n\njs_l <- polr(job_sat ~ use_tech_f + wrkhome_f + post_covid  + female + poc + age_group + work_type + years_job +degree, data=GSS, na.action = na.exclude, method = \"logistic\", weights= weights )\n\n\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!\n\n\nCode\njs_l_int<-polr(job_sat ~ use_tech_f + wrkhome_f + post_covid +use_tech_f:post_covid + wrkhome_f:post_covid + female + poc + age_group + work_type + years_job + degree, data=GSS, na.action = na.exclude, method = \"logistic\", weights= weights )\n\n\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!"
  },
  {
    "objectID": "posts/NJ_Adv_Quant.html#display-results",
    "href": "posts/NJ_Adv_Quant.html#display-results",
    "title": "U.S. Job Satisfaction: Impacts of Tech, Remote Work & Covid",
    "section": "Display results",
    "text": "Display results\nHere I display my results of my main effect model and interaction effect model side by side using msummary(). I renamed all the coefficients to the correct labels. I include stars = TRUE to display the significance level of each variable.\n\n\nCode\nmsummary(list(\"Main Effect\" = js_l,\n              \"Interaction\" = js_l_int),\n         coef_rename = c(\"use_tech_f2\" = \"Uses tech 26-50% per week\",\n                          \"use_tech_f3\" = \"Uses tech 51-75% per week\",\n                          \"use_tech_f4\" = \"Uses tech >75% per week\",\n                          \"wrkhome_f2\"  = \"WFH few times a year\",\n                          \"wrkhome_f3\" = \"WFH about once a month\",\n                          \"wrkhome_f4\" = \"WFH about once a week\",\n                          \"wrkhome_f5\" = \"WFH more than once a week\",\n                          \"wrkhome_f6\" = \"Mainly WFH\",\n                          \"post_covid1\" =  \"2022\",\n                          \"female\" = \"Female\",\n                          \"poc\" =  \"POC\",\n                           \"age_group\" =  \"Age group\",\n                          \"work_typeOn-call, work only when called to work\"  = \"On-call, work only when called to work\",\n                          \"work_typePaid by a temporary agency\" =  \"Paid by a temporary agency\",\n                          \"work_typeWork for contractor who provides workers/services\" =\n                           \"Work for contractor who provides workers/services\",\n                          \"work_typeRegular, permanent employee\"  = \"Regular, permanent employee\",\n                           \"years_job\" = \"Years worked at current job\",\n                            \"degree\" = \"Education Level\",\n                           \"use_tech_f2 × post_covid1\"  = \"Uses tech 26-50% per week X 2022\",\n                           \"use_tech_f3 × post_covid1\" =   \"Uses tech 51-75% per week X 2022\",\n                           \"use_tech_f4 × post_covid1\" =  \"Uses tech >75% per week X 2022\",\n                           \"wrkhome_f2 × post_covid1\"  = \"WFH few times a year X 2022\",\n                            \"wrkhome_f3 × post_covid1\" =  \"WFH about once a month X 2022\",\n                            \"wrkhome_f4 × post_covid1\" = \"WFH about once a week X 2022\",\n                            \"wrkhome_f5 × post_covid\" = \"WFH more than once a week X 2022\",\n                            \"wrkhome_f6 × post_covid\" = \"Mainly WFH X 2022\"), stars = TRUE)\n\n\n\n\n \n  \n      \n    Main Effect \n     Interaction \n  \n \n\n  \n    1|2 \n    −4.080*** \n    −4.031*** \n  \n  \n     \n    (0.201) \n    (0.212) \n  \n  \n    2|3 \n    −2.694*** \n    −2.645*** \n  \n  \n     \n    (0.177) \n    (0.190) \n  \n  \n    3|4 \n    −0.291+ \n    −0.239 \n  \n  \n     \n    (0.169) \n    (0.183) \n  \n  \n    Uses tech 26-50% per week \n    −0.169 \n    −0.235 \n  \n  \n     \n    (0.105) \n    (0.156) \n  \n  \n    Uses tech 51-75% per week \n    0.035 \n    0.062 \n  \n  \n     \n    (0.127) \n    (0.186) \n  \n  \n    Uses tech >75% per week \n    −0.350*** \n    −0.241+ \n  \n  \n     \n    (0.091) \n    (0.131) \n  \n  \n    WFH few times a year \n    0.158 \n    0.167 \n  \n  \n     \n    (0.146) \n    (0.213) \n  \n  \n    WFH about once a month \n    0.584*** \n    0.470* \n  \n  \n     \n    (0.167) \n    (0.222) \n  \n  \n    WFH about once a week \n    0.297* \n    0.464* \n  \n  \n     \n    (0.139) \n    (0.192) \n  \n  \n    WFH more than once a week \n    0.291* \n    0.403* \n  \n  \n     \n    (0.114) \n    (0.175) \n  \n  \n    Mainly WFH \n    0.390** \n    0.328 \n  \n  \n     \n    (0.123) \n    (0.230) \n  \n  \n    2022 \n    −0.050 \n    0.035 \n  \n  \n     \n    (0.070) \n    (0.131) \n  \n  \n    Female \n    −0.029 \n    −0.027 \n  \n  \n     \n    (0.070) \n    (0.070) \n  \n  \n    POC \n    −0.144+ \n    −0.145+ \n  \n  \n     \n    (0.079) \n    (0.079) \n  \n  \n    Age group \n    0.223*** \n    0.225*** \n  \n  \n     \n    (0.032) \n    (0.032) \n  \n  \n    On-call, work only when called to work \n    −0.191 \n    −0.188 \n  \n  \n     \n    (0.243) \n    (0.244) \n  \n  \n    Paid by a temporary agency \n    −1.618*** \n    −1.628*** \n  \n  \n     \n    (0.364) \n    (0.365) \n  \n  \n    Work for contractor who provides workers/services \n    −0.343 \n    −0.347 \n  \n  \n     \n    (0.221) \n    (0.222) \n  \n  \n    Regular, permanent employee \n    −0.533*** \n    −0.529*** \n  \n  \n     \n    (0.112) \n    (0.113) \n  \n  \n    Years worked at current job \n    −0.094* \n    −0.098* \n  \n  \n     \n    (0.044) \n    (0.044) \n  \n  \n    Education Level \n    −0.052+ \n    −0.053+ \n  \n  \n     \n    (0.032) \n    (0.032) \n  \n  \n    Uses tech 26-50% per week:2022 \n     \n    0.124 \n  \n  \n     \n     \n    (0.209) \n  \n  \n    Uses tech 51-75% per week:2022 \n     \n    −0.027 \n  \n  \n     \n     \n    (0.250) \n  \n  \n    Uses tech >75% per week:2022 \n     \n    −0.191 \n  \n  \n     \n     \n    (0.175) \n  \n  \n    WFH few times a year:2022 \n     \n    −0.027 \n  \n  \n     \n     \n    (0.289) \n  \n  \n    WFH about once a month:2022 \n     \n    0.224 \n  \n  \n     \n     \n    (0.333) \n  \n  \n    WFH about once a week:2022 \n     \n    −0.353 \n  \n  \n     \n     \n    (0.272) \n  \n  \n    WFH more than once a week:2022 \n     \n    −0.167 \n  \n  \n     \n     \n    (0.225) \n  \n  \n    Mainly WFH:2022 \n     \n    0.109 \n  \n  \n     \n     \n    (0.266) \n  \n  \n    Num.Obs. \n    3268 \n    3268 \n  \n  \n    AIC \n    6323.9 \n    6334.1 \n  \n  \n    BIC \n    6451.8 \n    6510.8 \n  \n  \n    RMSE \n    3.20 \n    3.20 \n  \n\n\n + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\n\n\n\n\nMain Effect model: Based on the model results, there is evidence that supports Hypothesis 1. Survey respondents who WFH about once a month (0.584 log odds) , WFH about once a week (0.297 log odds), WFH more than once a week (0.291 log odds), and Mainly WFH (0.390 log odds) are all significantly more likely (p < 0.05) to be satisfied with their jobs than survey respondents who never WFH. Additionally, there is evidence from the model results that supports Hypothesis 4. Survey respondents who use technology more than 75% of their total work time per week (−0.350 log odds) are significantly less likely (p <0.05) to be satisfied with their jobs than those who use technology less than 25% of their total work time per week. Hypothesis 3 is not supported based on the model results as ‘2022’ is not statistically different from 0 (−0.050 log odds , 0.70 SE). Interestingly, survey respondents who are Paid by a temporary agency (−0.533 log odds) or Regular, permanent employees (−1.618 log odds) are significantly less likely to be satisfied with their jobs than those who are Independent contractors. Other control variables that are statistically different from 0 at p < 0.05 include age group and Years worked at current job.\nInteraction model: Unfortunately, there is insufficient evidence that supports Hypotheses #2 and #5 as none of the interactions between working from home, the % of time using technology in a work week, and year (pre and post covid) are statistically different from 0 at p < 0.05."
  },
  {
    "objectID": "posts/NJ_Adv_Quant.html#create-predicted-probabilites-for-use_tech",
    "href": "posts/NJ_Adv_Quant.html#create-predicted-probabilites-for-use_tech",
    "title": "U.S. Job Satisfaction: Impacts of Tech, Remote Work & Covid",
    "section": "Create predicted probabilites for use_tech",
    "text": "Create predicted probabilites for use_tech\n\n\nCode\nlogit<-ggpredict(js_l, terms=\"use_tech_f\")"
  },
  {
    "objectID": "posts/NJ_Adv_Quant.html#graph-predicted-probabilities-of-use_tech",
    "href": "posts/NJ_Adv_Quant.html#graph-predicted-probabilities-of-use_tech",
    "title": "U.S. Job Satisfaction: Impacts of Tech, Remote Work & Covid",
    "section": "Graph Predicted probabilities of use_tech",
    "text": "Graph Predicted probabilities of use_tech\nI filtered out all non significant levels of use_tech. I also display the legend at the bottom of the graph for spacing purposes.\n\n\nCode\ncustom_levels <- c(1, 2, 3, 4)\ncustom_labels <- c(\"Not at all satisfied\", \"Not too satisfied\", \"Somewhat satisfied\", \"Very satisfied\")\n\n# Reassign variable with custom labels\nlogit$response.level <- factor(logit$response.level, levels = custom_levels, labels = custom_labels)  \n\n  \nlogit1 <- logit %>%\n    filter(logit$x %in% c(1, 4))\n#Filters for the variables we nee\n  \ncustom_order1 <- c('<=25% per week','>75% per week')  \n\nggplot(logit1, aes(x = x, y = predicted, color = response.level, group = response.level)) +\n     geom_point() +   theme_minimal(base_size = 10) +\n    labs(x = \"% of tech used per week\", y = \"Predicted Probability\", \n         title = \"Predicted Probability of Job Satisfaction with 95% CI\") +\n    labs(color = \"Job Satisfaction\") +\n    geom_errorbar(aes(ymin=conf.low, ymax=conf.high),\n                  linewidth=.3,    # Thinner lines\n                  width=.2)  + scale_x_discrete(labels = custom_order1)  + theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 1: Predicted probabilities of the reference and significant levels of % of tech used per week during work\nFigure 1 shows that survey respondents who use technology more than 75% of their total work time per week have a lower predicted probability of being very satisfied with their job (~55% probability of being very satisfied) than those who use technology less than 25% of their total work time per week (~ 62% probability of being very satisfied). The reason for this could be similar to the conclusion found in the literature review about technology: the increased amount of information workers have to deal with due to using more technology is much harder to handle than workers who do not use as much technology in the workplace."
  },
  {
    "objectID": "posts/NJ_Adv_Quant.html#graph-predicted-probabilities-of-wrkhome",
    "href": "posts/NJ_Adv_Quant.html#graph-predicted-probabilities-of-wrkhome",
    "title": "U.S. Job Satisfaction: Impacts of Tech, Remote Work & Covid",
    "section": "Graph Predicted probabilities of wrkhome",
    "text": "Graph Predicted probabilities of wrkhome\nI filtered out all non significant levels of wrkhome. I also display the legend at the bottom of the graph for spacing purposes.\n\n\nCode\nlogitW<-ggpredict(js_l, terms=\"wrkhome_f\")\n\nlogitW$response.level <- factor(logitW$response.level, levels = custom_levels, labels = custom_labels)\nlogitW1 <- logitW %>%\n    filter(logitW$x %in% c(1,3,4,5,6))\n\ncustom_order2 <- c('Never WFH','WFH about \\n once a month', 'WFH about \\n once a week',\n                   'WFH more \\n than once a week', 'Mainly WFH' )  \n\nggplot(logitW1, aes(x = x, y = predicted, color = response.level, group = response.level)) +\n     geom_point() +   theme_minimal(base_size = 10) +\n    labs(x = \"How Often Responents Work from Home \", y = \"Predicted Probability\", \n         title = \"Predicted Probability of Job Satisfaction with 95% CI\") +\n    labs(color = \"Job Satisfaction\") +\n    geom_errorbar(aes(ymin=conf.low, ymax=conf.high),\n                  linewidth=.3,    # Thinner lines\n                  width=.2)  + scale_x_discrete(labels = custom_order2)  + theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 2: Predicted probabilities of the reference and significant levels of how often respondents work from home\nFigure 2 shows that the survey respondents who Never WFH have a lower predicted probability of being very satisfied with their job (~ 60% probability of being very satisfied) than those who mainly WFH (~70% probability of being very satisfied). The reason for why this is the case could be that workers feel more comfortable in an at home work environment and favor the flexibility of working from home over working in person.\n\n\nCode\nlogitWT<-ggpredict(js_l, terms=\"work_type\")\n\nlogitWT$response.level <- factor(logitWT$response.level, levels = custom_levels, labels = custom_labels) \n\nlogitWT2 <- logitWT %>%\n    filter(logitWT$x %in% c('Independent contractor/consultant/freelance worker','Paid by a temporary agency','Regular, permanent employee'))\n\n#custom_order3 <- c('Independent contractor \\n /consultant/freelance \\n worker',\n#                   'On-call, \\n work only \\n when called to work','Paid by \\n a temporary \\n agency',\n#                   'Work for contractor \\n who provides \\n workers/services',\n#                   'Regular, \\n permanent employee' )\n\ncustom_order3 <- c('Independent contractor \\n /consultant/freelance \\n worker',\n                   'Paid by \\n a temporary \\n agency',\n                   'Regular, \\n permanent employee' )\n\nggplot(logitWT2, aes(x = x, y = predicted, color = response.level, group = response.level)) +\n     geom_point() +   theme_minimal(base_size = 10) +\n    labs(x = \"Work Type of Respondent \", y = \"Predicted Probability\", \n         title = \"Predicted Probability of Job Satisfaction with 95% CI\") +\n    labs(color = \"Job Satisfaction\") +\n    geom_errorbar(aes(ymin=conf.low, ymax=conf.high),\n                  linewidth=.3,    # Thinner lines\n                  width=.2)  + scale_x_discrete(labels = custom_order3) + theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 3: Predicted probabilities of the reference and significant levels of work type of respondent\nFigure 3 shows the survey respondents who are Independent contractor/consultant/ freelance workers have a much higher predicted probability of being very satisfied with their job (~62% probability of being very satisfied) than survey respondents who are Paid by a temporary agency (~25% probability of being very satisfied) and regular, permanent employees (~ 50% probability of being very satisfied). The reason Independent contractor/consultant/ freelance workers have a greater chance of being very satisfied with jobs could be because of how flexible their jobs are. Although their job security may not be as safe as other types of work arrangements, Independent contractor/consultant/ freelance workers are tasked with work that they excel and are interested in, which leads towards the belief that they are more satisfied with their jobs than the other significant work arrangements."
  },
  {
    "objectID": "templates/AboutTemplate.html",
    "href": "templates/AboutTemplate.html",
    "title": "Your Name",
    "section": "",
    "text": "##Instructions"
  },
  {
    "objectID": "templates/AboutTemplate.html#educationwork-background",
    "href": "templates/AboutTemplate.html#educationwork-background",
    "title": "Your Name",
    "section": "Education/Work Background",
    "text": "Education/Work Background"
  },
  {
    "objectID": "templates/AboutTemplate.html#r-experience",
    "href": "templates/AboutTemplate.html#r-experience",
    "title": "Your Name",
    "section": "R experience",
    "text": "R experience"
  },
  {
    "objectID": "templates/AboutTemplate.html#research-interests",
    "href": "templates/AboutTemplate.html#research-interests",
    "title": "Your Name",
    "section": "Research interests",
    "text": "Research interests"
  },
  {
    "objectID": "templates/AboutTemplate.html#hometown",
    "href": "templates/AboutTemplate.html#hometown",
    "title": "Your Name",
    "section": "Hometown",
    "text": "Hometown"
  },
  {
    "objectID": "templates/AboutTemplate.html#hobbies",
    "href": "templates/AboutTemplate.html#hobbies",
    "title": "Your Name",
    "section": "Hobbies",
    "text": "Hobbies"
  },
  {
    "objectID": "templates/AboutTemplate.html#fun-fact",
    "href": "templates/AboutTemplate.html#fun-fact",
    "title": "Your Name",
    "section": "Fun fact",
    "text": "Fun fact"
  },
  {
    "objectID": "templates/PostTemplate.html",
    "href": "templates/PostTemplate.html",
    "title": "Blog Post Template",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "templates/PostTemplate.html#instructions",
    "href": "templates/PostTemplate.html#instructions",
    "title": "Blog Post Template",
    "section": "Instructions",
    "text": "Instructions\nThis document provides yaml header inforamtion you will need to replicate each week to submit your homework or other blog posts. Please observe the following conventions:\n\nSave your own copy of this template as a blog post in the posts folder.\nEdit the yaml header to change your author name\ninclude a description that is reader friendly\nupdate the category list to indicate what category is the blog post, the data used, the main packages or techniques, your name, or any thing else to make your document easy to find\nedit as a normal qmd/rmd file\n\n\n\nCode\nx <- c(2,3,4,5)\nmean(x)\n\n\n[1] 3.5"
  },
  {
    "objectID": "templates/PostTemplate.html#rendering-your-post",
    "href": "templates/PostTemplate.html#rendering-your-post",
    "title": "Blog Post Template",
    "section": "Rendering your post",
    "text": "Rendering your post\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code.\n\n\n\n\n\n\nWarning\n\n\n\nBe sure that you have moved your *.qmd file into the posts folder BEFORE you render it, so that all files are stored in the correct location.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nOnly render a single file - don’t try to render the whole website!\n\n\n\n\n\n\n\n\nPilot Student Blogs\n\n\n\nWe are piloting a workflow including individual student websites with direted and limited pull requests back to course blogs. Please let us know if you would like to participate."
  },
  {
    "objectID": "templates/PostTemplate.html#reading-in-data-files",
    "href": "templates/PostTemplate.html#reading-in-data-files",
    "title": "Blog Post Template",
    "section": "Reading in data files",
    "text": "Reading in data files\nThe easiest data source to use - at least initially - is to choose something easily accessible, either from our _data folder provided, or from an online source that is publicly available.\n\n\n\n\n\n\nUsing Other Data\n\n\n\nIf you would like to use a source that you have access to and it is small enough and you don’t mind making it public, you can copy it into the _data file and include in your commit and pull request.\n\n\n\n\n\n\n\n\nUsing Private Data\n\n\n\nIf you would like to use a proprietary source of data, that should be possible using the same process outlined above. There may initially be a few issues. We hope to have this feature working smoothly soon!"
  }
]